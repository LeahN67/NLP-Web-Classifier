{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us4hAD1XXj3p"
      },
      "source": [
        "## Problem Statement\n",
        "\n",
        "\n",
        "\n",
        "In this case study, we are provided with URLs from 53000+ web pages. The objective is to build a classifier that can classify the web pages into their respective classes (Each web page can belong to only 1 class).\n",
        "\n",
        "<img src=\"classifier.png\" width=400 height=400>\n",
        "\n",
        "Basically given the complete url, predict the tag a web page belongs to out of 9 predefined tags as given below:\n",
        "* People profile\n",
        "* Conferences/Congress\n",
        "* Forums\n",
        "* News article\n",
        "* Clinical trials\n",
        "* Publication\n",
        "* Thesis\n",
        "* Guidelines\n",
        "* Others\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiowowhCXj3q"
      },
      "source": [
        "## Data Description\n",
        "\n",
        "The dataset contains the following features:\n",
        "* web page_ID: Unique ID for the web page (1,2,3.... )\n",
        "* Domain: Domain of the web page (Example: www.fiercepharma.com)\n",
        "* Url: Complete URL of the web page (Example: http://www.fiercepharma.com/marketing/tecfidera-gilenya-and-aubagio-s-3-way-battle-for-ms-share-about-to-get-more-interesting)\n",
        "* Tag: (Target) Tag (class) of the web page\n",
        "\n",
        "The objective here is to predict the class of the web page from the above mentioned 9 classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFSAYz5GXj3r"
      },
      "source": [
        "#### Components of a URL\n",
        "Let us have a look at each component of a URL to have better understanding of the data.\n",
        "\n",
        "<img src=\"url_comp.png\" width=600 height=600>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dJV7mTTXj3r"
      },
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "i6TABmleXj3s"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "color = sns.color_palette()\n",
        "from tqdm import tqdm\n",
        "from urllib.parse import urlparse\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.model_selection import cross_val_predict, GroupKFold\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from scipy.sparse import hstack\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5SKFeadXj3t"
      },
      "source": [
        "### Load Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nH8RAeE0Xj3u"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"webpage data.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juyJy4SPXj3v"
      },
      "source": [
        "### Basic Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ty-zyA_Xj3w"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wZYQy5KXj3x"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0psdx4jMXj3y"
      },
      "outputs": [],
      "source": [
        "df['Tag'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUyZWsaBXj3y"
      },
      "source": [
        "As mentioned in the problem statement, there are 9 separate categories. Let us have a look at a few samples from each category to hav"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPpjrjkOXj3y"
      },
      "source": [
        "#### Public Profiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nX8BMUzXj3z"
      },
      "outputs": [],
      "source": [
        "df[df['Tag'] == 'profile'].head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5G-Wwd6Xj3z"
      },
      "source": [
        "#### Conferences/Congress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgEflfa_Xj3z"
      },
      "outputs": [],
      "source": [
        "df[df['Tag'] == 'conferences'].head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yopHHCoyXj3z"
      },
      "source": [
        "#### Forum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Awh4F3osXj3z"
      },
      "outputs": [],
      "source": [
        "df[df['Tag'] == 'forum'].head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-vHAEEmXj30"
      },
      "source": [
        "#### News Articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51ZS41AYXj30"
      },
      "outputs": [],
      "source": [
        "df[df['Tag'] == 'news'].head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA78F-UEXj30"
      },
      "source": [
        "#### Clinical Trials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqimeMT8Xj30"
      },
      "outputs": [],
      "source": [
        "df[df['Tag'] == 'clinicalTrials'].head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy_AGyvlXj31"
      },
      "source": [
        "#### Publications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiXhnLApXj31"
      },
      "outputs": [],
      "source": [
        "df[df['Tag'] == 'publication'].head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0T_URc7Xj31"
      },
      "source": [
        "#### Thesis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x61QRA4LXj31"
      },
      "outputs": [],
      "source": [
        "df[df['Tag'] == 'thesis'].head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xPp1JzSXj31"
      },
      "source": [
        "#### Guidelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsQ6NRSrXj31"
      },
      "outputs": [],
      "source": [
        "df[df['Tag'] == 'guidelines'].head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okedzPimXj32"
      },
      "source": [
        "#### Others"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYtV2hjzXj32"
      },
      "outputs": [],
      "source": [
        "df[df['Tag'] == 'others'].head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgTmuGnFXj32"
      },
      "source": [
        "* Looking at the samples, we can see that there are a few words in the URLs that are appearing consistently for each category. For example: there is the word forum for URLs of tag forum, gid for the URLs of tag guidelines and cfm for conferences. \n",
        "* This implies that it would be a good idea to actually find out the frequency of each word and use that as a feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_IgS25IXj32"
      },
      "source": [
        "### Target Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYGDDcUgXj32"
      },
      "source": [
        "Let us look at the frequency distribution of all tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5D9bD15oXj32"
      },
      "outputs": [],
      "source": [
        "cnt_tag = df['Tag'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(cnt_tag.index, cnt_tag.values, alpha=0.8, color=color[3])\n",
        "plt.xticks(rotation='vertical')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roQ27kIaXj32"
      },
      "source": [
        "There are 2 major categories apart from others - news and publication. Public profiles, forums and conferences lie in the next bracket."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw4YQhqGXj33"
      },
      "source": [
        "### Understanding the common words used in the URLs: WordCloud\n",
        "\n",
        "Now I want to see how well the given sentiments are distributed across the train dataset. One way to accomplish this task is by understanding the common words by plotting wordclouds.\n",
        "\n",
        "A wordcloud is a visualization wherein the most frequent words appear in large size and the less frequent words appear in smaller sizes.\n",
        "\n",
        "Let’s visualize all the words our data using the wordcloud plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7H8onhObXj33"
      },
      "outputs": [],
      "source": [
        "all_words = ' '.join([text for text in df['Url']])\n",
        "from wordcloud import WordCloud\n",
        "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REMBDswZXj33"
      },
      "source": [
        "Overall, most of the URLs direct to healthcare pages as can be seen from the word cloud and there are words such as thesis, edu etc. which again imply that frequency of words should be an important feature for prediction.\n",
        "\n",
        "Now, a good idea would be to create word clouds for each specific cateogry."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95GT8UlbXj34"
      },
      "source": [
        "### Word Cloud for the tag - thesis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCiZsKyDXj34"
      },
      "outputs": [],
      "source": [
        "all_words = ' '.join([text for text in df[df['Tag'] == 'thesis']['Url']])\n",
        "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJcEdTPHXj34"
      },
      "source": [
        "Similarly, You can build word clouds for each category as well to get more idea of what the url looks like for each category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KUarUyzXj34"
      },
      "source": [
        "### Feature Extraction\n",
        "Here we simply have domain and URL and these are neither numeric nor categorical variable as each URL is unique. \n",
        "\n",
        "Here as learnt in the NLP module, we would need to extract information from text to create features and build a classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMxDiQMNXj34"
      },
      "source": [
        "The URLs in the dataset can be considered as a single string since the words in the URL have no spaces. Instead, there are 2 types of separators here '/' and '-'. We can replace these by spaces and we can get individual words this way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "njE14wuyXj34"
      },
      "outputs": [],
      "source": [
        "def clean_url(df):\n",
        "    df[\"Url\"] = df[\"Url\"].str.replace(\"/\", \" \")\n",
        "    df[\"Url\"] = df[\"Url\"].str.replace(\"-\", \" \")\n",
        "    df[\"Url\"] = df[\"Url\"].str.replace(\"https:\", \"\")\n",
        "    df[\"Url\"] = df[\"Url\"].str.replace(\"http:\", \"\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QeAZNfPMXj35"
      },
      "outputs": [],
      "source": [
        "df = clean_url(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59zXeQeZXj35"
      },
      "outputs": [],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFyDlgQeXj35"
      },
      "source": [
        "### Feature Extraction\n",
        "\n",
        "Now, that we have tokenised the words and done necessary cleaning. It is time to convert these to features. In the NLP module, we learned about various techniques:\n",
        "* Bag of Words\n",
        "* TFIDF (Term Frequency Inverse Document Frequency)\n",
        "* Word Vectors\n",
        "\n",
        "Here, we will demonstrate BOW features and TFIDF features. Sklearn provides functionality for both. \n",
        "\n",
        "Let's use that to create these features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd9RDFSlXj35"
      },
      "source": [
        "### Bag-of-Words Features\n",
        "\n",
        "The URL has has lot of abbreviations and for the same words so it could be a good idea to use create bag of word features from characters as well.\n",
        "\n",
        "<img src = \"image 2.png\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UNnecpO8Xj35"
      },
      "outputs": [],
      "source": [
        "# Word and character BOW on URLs\n",
        "vec_bow = CountVectorizer(ngram_range=(1, 3), min_df=400)\n",
        "vec_bow.fit(df['Url'])\n",
        "Url_bow = vec_bow.transform(df['Url'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IhVaBzjXj35"
      },
      "source": [
        "If you look at the documentation for Sklearn feature extraction, you will see that:\n",
        "* ngram_range helps to define the range of n for the ngrams\n",
        "* When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold min_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTjTMCQSXj35"
      },
      "source": [
        "## Model Building\n",
        "This is a multi-class classification problem and the metric that we will use here is weighted F1-Score. As discussed in the multi-class module, weighted F1 score basically assigns weights proportional to the class frequency in the train set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8neTJWn6Xj36"
      },
      "source": [
        "## Train Test Split\n",
        "\n",
        "Randomly spliting the dataset into train and test and checking performance will not be correct for this problem. Here's why:\n",
        "\n",
        "Let's say we have a domain ecommons.cornell.edu. This is basically Cornell University's digital repository and predominantly contain thesis class. Now, if this domain (ecommons.cornell.edu) and class (thesis) combination are contained in both train and test. Just on the basis of domain I can predict the class to be thesis but this model would not be useful and would not generalise well on let's say new thesis by a different domain.\n",
        "\n",
        "The Solution?\n",
        "\n",
        "\n",
        "The train and test data split should done based on Domain-Tag combination, such that no 2 URLs for the same class and domain are kept in the train and test respectively because in that case domain can be directly mapped to the tag and that would be a leakage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuX_OmKQXj36"
      },
      "source": [
        "## Model Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAF1DVdpXj36"
      },
      "source": [
        "For implementing the above logic. We will use Group K-Fold from Sklearn. In group K-Fold, the same group will not appear in two different folds. The folds are approximately balanced in the sense that the number of distinct groups is approximately the same in each fold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymyXfn7yXj36"
      },
      "outputs": [],
      "source": [
        "# Replicate train/test split strategy for cross validation\n",
        "df[\"target_str\"] = df[\"Domain\"].astype(str) + '_' + df[\"Tag\"].astype(str)\n",
        "# cvlist = list(GroupKFold(5).split(df, groups=df[\"target_str\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OreDFTWXj36"
      },
      "outputs": [],
      "source": [
        "df[\"target_str\"].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQrFeVXEXj36"
      },
      "outputs": [],
      "source": [
        "cvlist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfhZYMZOXj36"
      },
      "source": [
        "#### Convert tags from strings to numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBWU7flRXj36"
      },
      "outputs": [],
      "source": [
        "X = Url_bow\n",
        "\n",
        "TAG_DICT = {\"others\":1, \"news\": 2, \"publication\":3, \"profile\": 4,\n",
        "            \"conferences\": 5, \"forum\": 6, \"clinicalTrials\": 7,\n",
        "            \"thesis\": 8, \"guidelines\": 9}\n",
        "\n",
        "df['target'] = df.Tag.map(TAG_DICT)\n",
        "y = df[\"target\"].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "joHjB7BgXj37"
      },
      "outputs": [],
      "source": [
        "def cv_score(ml_model, df):\n",
        "    i = 1\n",
        "    cv_scores = []\n",
        "    X = df\n",
        "    \n",
        "    # Custom Cross validation based on group KFold\n",
        "    for df_index,test_index in cvlist:\n",
        "        print('\\n{} of Group kfold {}'.format(i,5))\n",
        "        xtr,xvl = X[df_index],X[test_index]\n",
        "        ytr,yvl = y[df_index],y[test_index]\n",
        "            \n",
        "        # Define model for fitting on the training set for each fold\n",
        "        model = ml_model\n",
        "        model.fit(xtr, ytr)\n",
        "        pred_probs = model.predict_proba(xvl)\n",
        "        label_preds = np.argmax(pred_probs, axis=1) + 1\n",
        "        \n",
        "        # Calculate scores for each fold and print\n",
        "        score = f1_score(yvl, label_preds, average=\"weighted\")\n",
        "        sufix = \"\"\n",
        "        msg = \"\"\n",
        "        msg += \"Weighted F1 Score: {}\".format(score)\n",
        "        print(\"{}\".format(msg))\n",
        "         \n",
        "         # Save scores\n",
        "        cv_scores.append(score)\n",
        "        i+=1\n",
        "    return cv_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gB4jFFG0Xj37"
      },
      "source": [
        "### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "mwa5-lwIXj37"
      },
      "outputs": [],
      "source": [
        "cv_score(MultinomialNB(alpha=.01), Url_bow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep0DkcISXj37"
      },
      "source": [
        "### Character N-Grams\n",
        "\n",
        "These scores are low. Since, the URLs are not regular sentences. It would be a good idea to also build features using character ngrams as well. \n",
        "\n",
        "For sequences of characters, the 3-grams that can be generated from \"good morning\" are \"goo\", \"ood\", \"od \", \"d m\", \" mo\", \"mor\"\n",
        "\n",
        "Let's do that and check performance again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTebwJ9aXj37"
      },
      "outputs": [],
      "source": [
        "# Word and character BOW on URLs\n",
        "vec1 = CountVectorizer(analyzer='char', ngram_range=(1, 5), min_df=500)\n",
        "vec2 = CountVectorizer(analyzer='word', ngram_range=(1, 3), min_df=400)\n",
        "vec_bow = FeatureUnion([(\"char\", vec1), (\"word\", vec2)])\n",
        "vec_bow.fit(df['Url'])\n",
        "Url_bow = vec_bow.transform(df['Url'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJ-uZlc5Xj37"
      },
      "outputs": [],
      "source": [
        "cv_score(MultinomialNB(alpha=.01), Url_bow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYHVvjvsXj37"
      },
      "source": [
        "We see significant improvement by using the character N-Grams. Now, lets try the TFIDF features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZpjVsjKXj37"
      },
      "source": [
        "### TFIDF Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "C4mCENBIXj38"
      },
      "outputs": [],
      "source": [
        "# Word and character TFIDF on URLs\n",
        "vec1 = TfidfVectorizer(analyzer='char', ngram_range=(1, 5), min_df=500)\n",
        "vec2 = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=400)\n",
        "vec_tfidf = FeatureUnion([(\"char\", vec1), (\"word\", vec2)])\n",
        "vec_tfidf.fit(df['Url'])\n",
        "Url_tfidf = vec_tfidf.transform(df['Url'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nnfTxr9Xj38"
      },
      "outputs": [],
      "source": [
        "nb = cv_score(MultinomialNB(alpha=.01), Url_tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiT_HMX_Xj38"
      },
      "source": [
        "### Logistic Regression\n",
        "\n",
        "We are getting better performance from TFIDF features. Let's go with that and try logistic regression now. Here, I have used class weight as balanced. This specifically changes the weights of samples inversely proportional to theit frequency meaning the classes which have less number of samples will have more weight."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cqFERxlXj38"
      },
      "outputs": [],
      "source": [
        "log_reg = cv_score(LogisticRegression(C=0.1,class_weight=\"balanced\"), Url_tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR1iIkbgXj38"
      },
      "source": [
        "### Tree Based Methods\n",
        "\n",
        "Next, we will try tree based methods to check performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrTvkSQwXj38"
      },
      "source": [
        "### Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQpRUd3LXj38"
      },
      "outputs": [],
      "source": [
        "dtree = cv_score(DecisionTreeClassifier(min_samples_leaf=25, min_samples_split=25), Url_tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H270RV8_Xj38"
      },
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psS-qTfVXj38"
      },
      "outputs": [],
      "source": [
        "rf_params = {'random_state': 0, 'n_jobs': -1, 'n_estimators': 100, 'max_depth': 50, 'n_jobs': -1}\n",
        "rf = cv_score(RandomForestClassifier(**rf_params), Url_tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "XURXbDWYXj39"
      },
      "outputs": [],
      "source": [
        "results_df = pd.DataFrame({'Random Forest':rf, 'Decision Tree': dtree, 'Logistic Regression': log_reg, 'Naive Bayes':nb})\n",
        "results_df.plot(y=[\"Random Forest\", \"Decision Tree\",\"Logistic Regression\",\"Naive Bayes\"], kind=\"bar\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2b3opEMXj39"
      },
      "source": [
        "### Conclusions\n",
        "* Logistic Regression is getting the best performance.\n",
        "* Interestingly Tree Based methods are performing badly.\n",
        "* The scores are not very stable due to the large number of classes and few samples"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "colab": {
      "name": "Web_Page_Classification_Project.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}